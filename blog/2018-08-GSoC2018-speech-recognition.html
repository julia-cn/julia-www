<!DOCTYPE html>
<html>

<?php 
$title = "GSoC 2018和Flux模型动物园的语音识别：结论";
$keywords = "GSoC 2018和Flux模型动物园的语音识别：结论";
$description = "";
$active_menu = "blog";
require_once('../_common/head.html'); ?>

<body>

  <?php require_once('../_common/nav.html'); ?>

  <br><br>

  <div class="container">

    <div class="row">
      <div class="col-12 col-lg-8 offset-lg-2">
        <h1></h1>

        <div id="blogpost">
          <h1>GSoC 2018和Flux模型动物园的语音识别：结论</h1>

          <p class="metadata">
            <span class="timestamp">2018年8月14日</span> | <span class="author"><a href="https://github.com/maetshju">Matthew C. Kelley</a></span>
          </p>

          <p>
            在这里，我们处于Google Summer of Code 2018的另一端。这是一次具有挑战性和教育性的和无可替代的一种体验，我很感谢Julia社区，
            特别是我的导师<a href="https://github.com/mikeinnes">@MikeInnes</a>，感谢他支持我。我学到了很多东西，比以前更熟练掌握神经网络，
            我学会了如何进行基本的GPU编程，这对我的学术生涯非常有用。
            </p>

          <p>
            本博文的其余部分将总结我的项目和我整个夏天所做的工作，然后记录自己还有哪些待完成的工作，
            最后简要介绍如何运行我编写的代码来试用它并且能够为你自己所用。
          </p>

          <h1 id="have-you-ever-wanted-your-computer-to-understand-speech">你曾经想过让你的电脑听得懂你的话吗？</h1>

          <p>语音识别目前在许多科技公司中流行。 例如，谷歌(Google Home)和亚马逊(Alexa)分别大力推动他们的独立语音助手产品。如果没有功能性语音识别，这些产品将胎死腹中</p>

          <p>
            不幸的是，对于许多研究人员和潜在的语音识别爱好者来说，
            语音识别系统的文档似乎没有图像识别系统那么多。
            这个2018年谷歌夏季代码项目的目标是将一些语音识别模型贡献给<a href="https://github.com/FluxML/model-zoo">Flux模型动物园</a>，这样就会有一些免费的模型供其他人使用。
          </p>

          <p>
            在项目结束时，训练出了两种不同的模型。第一种是使用称为连接时间分类（CTC）的损失函数的一种相当新的方法（Graves等，2006）。可实践的模型来自Zhang等人。（2017），它使用卷积层来学习数据中的时间依赖性，这与使用复发层的CTC损失的传统方法不同。这是一个层数非常深的网络，作者认为它允许它学习时间依赖性。
          </p>

          <p>第二个网络是一种旧式的框架识别模型，灵感来自Graves＆Schmidhuber（2005）。它预测通过网络传递的每个音频块的类，并使用分类交叉熵作为其丢失函数。它将作为CTC网络的基准进行比较。</p>

          <p>对于那些不熟悉语音识别系统的人来说，从声学到电话标签的映射仍然是一个未解决的问题，因为没有人仅仅在音频帧的标签识别上达到95％或99％的准确度 因此，报告的准确度似乎并不令人满意（CTC网络当然也是如此），但这也是语音识别系统的长期的诟病。</p>

          <h1 id="results-of-the-ctc-network">CTC网络的结果</h1>

          <p>
            一旦分解为步骤，该项目的主要任务是实现网络架构并在Flux和Julia中实现CTC损失功能。这两项任务的朴素实现变得简单，但性能不适合能够训练网络。回想起来，提高网络的计算效率并不困难，因为它只需要在Flux的<code class="highlighter-rouge">Chain</code>函数中添加一个<code class="highlighter-rouge">reshape</code>调用来将卷积层连接到完全连接的层。
          </p>

          <p>
            真正的试验是让CTC损失正确有效地运行。在最终决定使用百度的CTC GPU实现的简单端口<a href="https://github.com/baidu-research/warp-ctc">warp-ctc</a>之前，我起初在讨论CPU实现。这是我第一次尝试编写GPU内核，我学到了很多东西。但是在完成移植内核几周后，我有一个有关丢失功能的GPU实现。或者我想。我花了几周时间尝试使用各种优化器进行不同的训练配置和例程，但是我无法让网络输出超出空白语音标签类别的预测。我在<a href="https://maetshju.github.io/update3.html">几篇</a>博文中写过<a href="https://maetshju.github.io/update4.html">这篇</a>文章。
          </p>

          <p>
            事实证明，我的实现中存在轻微的错误，这些错误源自百度的warp-ctc库本身。正如我在撰写<a href="https://maetshju.github.io/update5.html">关于此错误的博客文章</a>时所做的那样，我不知道在其他百度代码的上下文中是否实际上是一个错误。但是，在修复错误后，我发现代码中的损失显着减少。具体来说，有一段代码被评估为：
          </p>

          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>β</mi>
            <mo>(</mo>
            <mi>t</mi>
            <mo>,</mo>
            <mi>u</mi>
            <mo>)</mo>
            <mo>=</mo>
            <msubsup>
              <mi>y</mi>
              <mrow>
                <mi>l</mi>
                <msub>
                  <mo>'</mo>
                  <mi>u</mi>
                </msub>
              </mrow>
              <mrow>
                <mi>t</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
            </msubsup>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mi>u</mi>
              </mrow>
              <mrow>
                <mi>g</mi>
                <mo>(</mo>
                <mi>u</mi>
                <mo>)</mo>
              </mrow>
            </munderover>
            <mi>β</mi>
            <mo>(</mo>
            <mi>t</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo>,</mo>
            <mi>i</mi>
            <mo>)</mo>
            <mo> </mo>
            <mo>,</mo>
          </math>

          <p>when it should have evaluated to</p>

          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>β</mi>
            <mo>(</mo>
            <mi>t</mi>
            <mo>,</mo>
            <mi>u</mi>
            <mo>)</mo>
            <mo>=</mo>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mi>u</mi>
              </mrow>
              <mrow>
                <mi>g</mi>
                <mo>(</mo>
                <mi>u</mi>
                <mo>)</mo>
              </mrow>
            </munderover>
            <mi>β</mi>
            <mo>(</mo>
            <mi>t</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo>,</mo>
            <mi>i</mi>
            <mo>)</mo>
            <msubsup>
              <mi>y</mi>
              <mrow>
                <mi>l</mi>
                <msub>
                  <mo>'</mo>
                  <mi>i</mi>
                </msub>
              </mrow>
              <mrow>
                <mi>t</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
            </msubsup>
            <mo> </mo>
            <mo>.</mo>
          </math>

          <p>More information on this is available in <a href="https://maetshju.github.io/update5.html">my previous
              post on it</a>. Making this change caused the network to finally output label predictions for each time
            step of data. The labels don’t necessarily always make sense, but it is at least making predictions.</p>

          <p>And this is where the speech recognition system is sitting right now. The network architecture should be
            implemented correctly, and the loss function would seem to be running correctly now. That is, the network
            runs and learns, to an extent. You can assess for yourself how well it’s learning based off the example
            output below:</p>

          <p><strong>Textual transcription</strong></p>

          <blockquote>
            <p>The reasons for this dive seemed foolish now.</p>
          </blockquote>

          <p><strong>Target sequence of phones</strong></p>

          <blockquote>
            <p>h# dh ix r iy z ax n z f axr dh ih s dcl d ay v s iy m dcl d f uw l ix sh epi n aw h#</p>
          </blockquote>

          <p><strong>Predicted sequence of phones</strong></p>

          <blockquote>
            <p>h# pau w iy bcl r iy ux z bcl b iy bcl b uw z ay n pcl p z iy n dcl d v w iy er h#</p>
          </blockquote>

          <p>The phone error rate for this prediction compared to the target was approximately 84%. This model is
            obviously not ready to be added to the model zoo, as it is not performing well.</p>

          <h1 id="results-of-the-framewise-network">框架式网络的结果</h1>

          <p>The main task for the framewise network was to find how to efficiently train the network. In terms of
            recognition results, the framewise network fared much better than the CTC network. In just two epochs of
            training, it reached approximately <strong>53.5% label prediction accuracy on the test set</strong>.
            Obviously, the network should be trained longer than that to achieve similar performance as Graves &
            Schmidhuber, but it’s noteworthy that it’s outperforming the CTC network even at such an early stage. The
            discrepancy between this network and the CTC network’s performance is astounding. Even though the task of
            the CTC network is more difficult, it is striking, if not surprising, that allowing the network to exploit
            extra information in the training data grants an improvement of this magnitude.</p>

          <h1 id="what-remains-to-be-done">剩下要做的事</h1>

          <p><strong>The framewise network is for all intents and purposes done.</strong> It should suit its purpose as
            a demonstration network, and it comes pretty close to matching Graves’s reported performance for the
            network. If it were desired to have it run faster, it could be made to work with batching and run on the
            GPU.</p>

          <p><strong>For the CTC network, there needs to be an investigation as to why it is not learning to perform to
              the degree Zhang et al. report.</strong> The following figure is a plot of the training and validation
            loss over time from the first time running the network with the corrected loss function mentioned above.
            The optimizer was AMSGrad with a learning rate of $10^{-4}$. It is characteristic of the behavior I’ve seen
            with other optimizers and training configurations, whether they are fresh starts or continuing previous
            training.</p>

          <p><img src="/images/blog/2018-08-14-GSoC2018-speech-recognition/ctcloss.png" alt="CTC loss over time. Note that the y-axis is presented in logarithmic scale."></p>

          <p>The validation loss is settling at a suboptimal level, even though the training loss continues to
            decrease. I am unsure at this juncture what the cause of this behavior may be, since I have endeavored to
            hew as closely as possible to the implementation details given by Zhang et al. I don’t believe it is in the
            CTC function since I have tested the CTC loss function several times against hand-worked solutions and seen
            it produce correct results; similarly, the gradients it provided allowed the network to fit one training
            example to a near-zero level of loss, so it would seem the function is providing a loss signal reliable
            enough to minimize the loss. It is possible that how I called the backpropagation needs to be investigated
            to see if the loss values from within a batch were being composed properly.</p>

          <h1 id="running-the-models">运行模型</h1>

          <p>Running the training procedure for the models should be straightforward. Make sure that the WAV, Flux,
            CuArrays, JLD, and BSON packages are installed. As well, install <a href="https://github.com/maetshju/MFCC.jl">the
              fork I’ve made of the MFCC package</a> (which only updates one line to make a function run on Julia 0.6).
            Start by cloning the Git repository for the project:</p>

          <blockquote>
            <p>$ git clone https://github.com/maetshju/gsoc2018.git</p>
          </blockquote>

          <p>The user will need to download the TIMIT speech corpus from the Linguistic Data Consortium, as I discussed
            in the first section of <a href="https://maetshju.github.io/speech-features.html">this previous blog post</a>.</p>

          <h2 id="ctc-model">CTC模型</h2>

          <p>Navigate into the <code class="highlighter-rouge">speech-cnn</code> folder. To extract the data from the
            TIMIT corpus, use the <code class="highlighter-rouge">00-data.jl</code> script. More information on this
            script can be found in <a href="https://maetshju.github.io/speech-features.html">the blog post dedicated to
              it</a>.</p>

          <blockquote>
            <p>$ julia 00-data.jl</p>
          </blockquote>

          <p>Now, to train the network, run the <code class="highlighter-rouge">01-speech-cnn.jl</code> script. Make
            sure you’ve removed the <code class="highlighter-rouge">README.md</code> files from the data folders, if
            you downloaded them.</p>

          <blockquote>
            <p>$ julia 01-speech-cnn.jl</p>
          </blockquote>

          <p>Note that it is essentially necessary to have a GPU to train the network on because the training process
            is extremely slow on just the CPU. Additionally, the script calls out to the GPU implementation of the CTC
            algorithm, which will fail without a GPU. The script will likely take over a day to run, so come back to it
            later. After the script finishes, the model should be trained and ready for use in making predictions.</p>

          <h2 id="framewise-model">Framewise模型</h2>

          <p>Navigate into the <code class="highlighter-rouge">speech-blstm</code> folder. To extract the data from the
            TIMIT corpus, use the <code class="highlighter-rouge">00-data.jl</code> script.</p>

          <blockquote>
            <p>$ julia 00-data.jl</p>
          </blockquote>

          <p>Now, to train the network, run the <code class="highlighter-rouge">01-speech-blstm.jl</code> script. Make
            sure you’ve removed the <code class="highlighter-rouge">README.md</code> files from the data folders, if
            you downloaded them.</p>

          <blockquote>
            <p>$ julia 01-speech-blstm.jl</p>
          </blockquote>

          <p>This network trains reasonably fast on the CPU, so GPU functionality was not implemented.</p>

          <h1 id="get-the-code">获取源代码</h1>

          <p>The code written during this project may be found <a href="https://github.com/maetshju/gsoc2018">on my
              GitHub</a>. There are also several pull requests that were made to contribute code to the larger package
            ecosystem:</p>

          <ul>
            <li><a href="https://github.com/FluxML/Flux.jl/pull/306">Adding epsilon term to Flux’s binary cross entropy
                loss</a></li>
            <li><a href="https://github.com/FluxML/Flux.jl/pull/342">Adding CTC loss to Flux</a></li>
            <li><a href="https://github.com/FluxML/model-zoo/pull/50">Adding the framewise speech recognition model to
                the Flux model zoo</a></li>
          </ul>

          <h1 id="references">参考文献</h1>

          <p>Graves, A., & Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other
            neural network architectures. <em>Neural Networks, 18</em>(5-6), 602-610.</p>

          <p>Graves, A., Fernández, S., Gomez, F., & Schmidhuber, J. (2006). Connectionist temporal classification:
            Labelling unsegmented sequence data with recurrent neural networks. In <em>Proceedings of the 23rd
              international conference on machine learning</em> (pp. 369-376). ACM.</p>

          <p>Zhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Bengio, C. L. Y., & Courville, A. (2017). Towards
            end-to-end speech recognition with deep convolutional neural networks. <em>arXiv preprint arXiv:1701.02720</em>.</p>


        </div>



      </div>
    </div>
  </div>

  <br>

  <?php require_once('../_common/foot.html'); ?>



</body>

</html>