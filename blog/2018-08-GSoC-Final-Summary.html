<!DOCTYPE html>
<html>

<?php 
$title = "GSOC 2018：使用Flux强化学习和生成模型";
$keywords = "";
$description = "";
$active_menu = "blog";
require_once('../_common/head.html'); ?>

<body>


  <?php require_once('../_common/nav.html'); ?>

  <br><br>

  <div class="container">

    <div class="row">
      <div class="col-12 col-lg-8 offset-lg-2">

        <article id="blogpost">
          <h1>GSOC 2018：使用Flux强化学习和生成模型</h1>

          <p class="metadata">
            <span class="timestamp">2018年8月6日</span> | <span class="author"><a href="https://github.com/tejank10">Tejan Karmali</a></span>

          </p>

          <p>Hello, world!</p>

          <p>
            在这篇文章中，我将简要总结一下我在今年夏天为GSOC工作的机器学习模型。我致力于丰富用<a href="https://github.com/julia/julialang">Julia</a>编写的机器学习库<a
              href="https://github.com/FluxML">Flux.jl</a>的模型动物园。我的项目包括强化学习和计算机视觉模型。
          </p>

          <p>该项目分布在这4个代码库中。</p>
          <ol>
            <li><a href="https://github.com/tejank10/Flux-baselines">Flux-baselines</a></li>
            <li><a href="https://github.com/tejank10/AlphaGo.jl">AlphaGo.jl</a></li>
            <li><a href="https://github.com/tejank10/model-zoo/tree/GAN">GAN 模型</a></li>
            <li><a href="https://github.com/tejank10/model-zoo/tree/DNI">DNI 模型</a></li>
          </ol>

          <p>在这个过程中，我可以实现我的大部分目标。我不得不跳过其中的几个，也做了一些没有计划的模型。下面，我将从资源库的角度来讨论这些问题。</p>

          <h3 id="1-flux-baselines">1. <a href="https://github.com/tejank10/Flux-baselines">Flux-baselines</a></h3>

          <p>
            通量基线是各种深层强化学习模型的集合。这包括DeepQ网络公司、Actor-Critic公司和DDPG公司。
          </p>

          <p>
            一个RL问题的基本结构如下：有一个环境，比方说乒乓球游戏就是我们的环境。环境可能包含许多相互作用的对象。在乒乓球中有三个物体：一个球和两个球拍。环境是有状态的。就环境中物体的各种特征而言，这就是环境中的现状。这些特征可以是位置、速度、颜色等。用于修饰或说明it中的对象。需要选择操作才能在环境中进行移动并获得下一个状态。直到比赛结束，我们都会选择行动。RL模型基本上可以找到需要选择的操作。
          </p>

          <p>
            在过去的几年里，深度强化学习得到了很大的普及。
            论文通过深思熟虑后对人的水平控制进行强化学习，没有回首。
            它将RL中的先进技术与深度学习相结合，得到了一个具有超人性能的AI球员。
            我在GSOC前阶段做了基本<a href="https://github.com/tejank10/Flux-baselines/blob/master/dqn/dqn.jl">DQN</a>和<a href="https://github.com/tejank10/Flux-baselines/blob/master/dqn/double-dqn.jl">双DQN</a>，然后在GSOC上做了<a
              href="https://github.com/tejank10/Flux-baselines/blob/master/dqn/duel-dqn.jl"></a>Duel DQN</a>。
          </p>

          <p>
            <a href="https://github.com/tejank10/Flux-baselines/blob/master/actor-critic/a2c.jl">A2C模型</a>的思想不同于DQN模型。
            A2C属于“演员-评论家”型。在AC模型中，我们有两种神经网络：策略网络和价值网络。
            策略网络接受游戏状态并返回操作空间上的概率分布。
            Value Nework将使用策略网络选择的状态和操作作为输入，并确定该操作是否适合该状态。
          </p>

          <p>
            当需要选择的动作分布在一个连续的空间上时，<a href="https://github.com/tejank10/Flux-baselines/tree/master/ddpg">DDPG</a>特别有用。
            您可能想到的一个可能的解决方案是，如果我们将操作空间离散化会怎样？
            如果我们把它缩小到很小的范围，我们最终会有大量的行动。
            如果我们分散它，那么我们就丢失了重要的数据。
          </p>

          <p>
            <img src="https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/ddpg.png" alt="DDPG">
            <br>
            DDPG: Score 对比 Episodes
          </p>

          <p>
            其中一些模型已经部署在Flux的<a href="https://fluxml.ai/experiments">网站</a>上。
            <a href="https://fluxml.ai/experiments/cartPole/">CartPole示例</a>已接受过深度Q网络方面的培训。几天后还将添加一个Atari-Pong示例。
            它是在DUEL-DQN上训练的。<a href="https://www.youtube.com/watch?v=L3pqMUDVrT0">以下</a>是使用Flux进行培训的Pong演示。
          </p>
          
          <h4 id="targets-achieved">已实现的目标</h4>
          <ol>
            <li><a href="https://github.com/tejank10/Flux-baselines/blob/master/actor-critic/a2c.jl">Advantage
                Actor-Critic</a></li>
            <li><a href="https://github.com/tejank10/Flux-baselines/blob/master/dqn/duel-dqn.jl">Duel DQN</a></li>
          </ol>

          <h4 id="extra-mile">拓展</h4>
          <ol>
            <li><a href="https://github.com/tejank10/Flux-baselines/tree/master/ddpg">DDPG</a></li>
            <li><a href="https://github.com/tejank10/Flux-baselines/blob/master/dqn/prioritized-replay-dqn.jl">Prioritized
                DQN</a></li>
          </ol>

          <h4 id="future-work">未来的工作</h4>
          <ol>
            <li>添加更多不同的模型，特别是在过去18个月中出现的模型。</li>
            <li>创建一个接口，以便从 <a href="https://github.com.JuliaML/OpenAIGym.jl">OpenAIGym.jl</a> 轻松地训练和测试任何环境。</li>
          </ol>

          <h3 id="2-alphagojl">2. <a href="https://github.com/tejank10/AlphaGo.jl">AlphaGo.jl</a></h3>
          <p>
            GSOC第二阶段的这个小型项目是最具挑战性的部分。AlphaGoZero是GoogleDeepMind的一项突破性人工智能。围棋是一种人工智能，它被认为是世界上最大的围棋之一，这主要是因为它可以导致的状态的数量。AlphaGoZero击败了世界上最好的围棋选手。alphaFo.jl的目标是在GO上实现AlphaGo Zero算法产生的结果，并在任何零和游戏中获得类似的结果。
          </p>

          <p>
            现在，我们有一个在Julia训练AlphaGo零模型的包！训练模型非常简单。
            我们只需要传递训练参数，我们想要训练模型的环境，然后再使用它。
            有关AlphaGo.jl的更多信息，请参阅<a href="https://tejank10.github.io/jekyll/update/2018/07/08/GSoC-Phase-2.html">博客文章</a>。
          </p>

          <h4 id="targets-achieved-1">已实现的目标</h4>
          <ol>
            <li>围棋</li>
            <li>蒙特卡罗树搜索</li>
          </ol>

          <h4 id="targets-couldnt-achieve">目标无法实现</h4>
          <ol>
            <li>不能很好的训练模型</li>
          </ol>

          <h4 id="extra-mile-1">拓展</h4>
          <ol>
            <li>Gomoku游戏的算法测试(因为游戏比较简单)</li>
          </ol>

          <h4 id="future-work-1">今后的工作</h4>
          <ol>
            <li>在任何游戏中训练模型</li>
            <li>AlphaChess</li>
          </ol>

          <h3 id="3-generative-adversarial-networks">3. <a href="https://github.com/tejank10/model-zoo/tree/GAN/vision/mnist">生成性对抗网络</a></h3>
          <p>
            GANS在学习任何数据的底层表示方面都非常成功。通过这样做，它可以复制一些假数据。例如，在MNIST人类手写数字数据集上训练的GANS可以生成一些与MNIST中的图像非常相似的假图像。这些神经网络在图像编辑中有着广泛的应用。它可以从图像中删除某些功能，添加一些新功能；具体取决于数据集。GANS包括两个网络：产生器和鉴别器。生成器的目标是生成伪图像，而鉴别器的目标是将生成器生成的假图像与数据集中的真实图像区分开来。
          </p>

          <p><img src="https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/lsgan.gif" alt="LSGAN"
              width="170px">
            <img src="https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/dcgan.gif" alt="DCGAN"
              width="170px">
            <img src="https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/giphy.gif" alt="WGAN"
              width="170px">
            <img src="https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/made.gif" alt="MADE"
              width="170px">
            <br>
                           LSGAN                               DCGAN                               WGAN                               MADE</p>

          <h4 id="targets-acheived">已实现的目标</h4>
          <ol>
            <li><a href="https://github.com/FluxML/Flux.jl/pull/311"><code class="highlighter-rouge">ConvTranspose</code>
                layer</a></li>
            <li><a href="https://github.com/tejank10/model-zoo/blob/GAN/vision/mnist/dcgan.jl">DCGAN</a></li>
          </ol>

          <h4 id="extra-mile-2">拓展</h4>
          <ol>
            <li><a href="https://github.com/tejank10/model-zoo/blob/GAN/vision/mnist/lsgan.jl">LSGAN</a></li>
            <li><a href="https://github.com/tejank10/model-zoo/blob/GAN/vision/mnist/wgan.jl">WGAN</a></li>
          </ol>

          <h4 id="future-work-2">今后的工作</h4>
          <ol>
            <li>更多的GAN模型，如infoGan，CycleGan开始</li>
            <li>Gans中的一些很酷的动画</li>
            <li>用于训练和生成带有数据集的图像的数据管道，并以GAN类型作为输入。</li>
          </ol>

          <h3 id="4-decoupled-neural-interface">4. <a href="https://github.com/tejank10/model-zoo/tree/DNI/vision/mnist/dni.jl">解耦神经接口
          </a></h3>

          <p>去耦神经网络接口是一种新的训练模型的技术。它不使用从输出层一直到输入层的反向传播。相反，它使用了一个技巧来“估计”梯度。它有一个小的线性层神经网络来预测梯度，而不是运行反向传播而不是找到真正的梯度。这种模型的优点是可以并行化。这种技术在精度上略有下降，但如果将网络中的各层进行并行化，则可以提高速度。</p>

          <p><img src="https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/loss.png" alt="loss"
              width="362.5px">
            <img src="https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/acc.png" alt="loss"
              width="362.5px"></p>

          <h4 id="targets-achieved-2">已实现的目标：</h4>
          <ul>
            <li><a href="https://github.com/tejank10/model-zoo/tree/DNI/vision/mnist/dni.jl">DNI 模型</a></li>
          </ul>

          <h2 id="conclusion">收尾</h2>

          <p>在过去的三个月里，我学到了很多关于强化学习，尤其是AlphaGo的知识。我训练了几天的RL模型，终于看到它工作得很好！我遇到了培训模型所面临的问题，并学会了如何克服它们。总而言之，作为一名有抱负的ML工程师，这三个月是最有成效的几个月。我很高兴我能达到我的大部分目标。我已经做了一些额外的模型，以弥补我不能达到的目标。</p>

          <h2 id="acknowledgements">鸣谢</h2>
          <p>
            我真的要感谢我的导师<a href="https://github.com/MikeInnes">Mike Innes</a>指导我完成了整个项目，还感谢<a href="https://github.om/jekbradbury">James Bradbury</a>为改进强化学习模型中的代码提供了宝贵的投入。我还要感谢<a href="https://github.com/roboneet">Neethu Mariya Joy</a>在网络上部署了经过培训的模型。最后但并非最不重要的是，朱莉娅项目和NumFOCUS：赞助我和所有其他JSOC学生朱莉亚康‘18伦敦。
          </p>
        </article>
      </div>
    </div>
  </div>

  <br>

  <?php require_once('../_common/foot.html'); ?>



</body>

</html>